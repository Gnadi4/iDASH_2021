# iDASH_2021 track 2 challenge code

![image](https://user-images.githubusercontent.com/29485153/143258076-1d0d5225-72ca-40d5-bd59-9ad684f82e82.png)

iDASH ì±Œë¦°ì§€ ì†Œê°œ í˜ì´ì§€ : [http://www.humangenomeprivacy.org/2021/competition-tasks.html](http://www.humangenomeprivacy.org/2021/competition-tasks.html)

### ì±Œë¦°ì§€ ì¼ì •

![image](https://user-images.githubusercontent.com/29485153/143258301-762a100e-7107-49da-bee4-84d02319fb73.png)

### Introduction

- í•´ë‹¹ ê³¼ì œëŠ” ì£¼ì–´ì§„ ë°”ì´ëŸ¬ìŠ¤ ê²Œë†ˆ(ì¦‰, COVID19 ê²Œë†ˆ)ì„ 4ê°€ì§€ ë‹¤ë¥¸ ê· ì£¼ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ì•ˆì „í•œ ë°©ë²•ì„ ê°œë°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤

### Track Description

- ê° íŒ€ì—ê²Œ ê°ê°ì˜ ê· ì£¼ ì •ë³´ì™€ í•¨ê»˜ 4ê°œì˜ ë‹¤ë¥¸ ê· ì£¼ì— ëŒ€í•œ 2000ê°œ(ì´ 8000ê°œì˜ ê²Œë†ˆ)ì˜ ì „ì²´ ê²Œë†ˆì´ í¬í•¨ëœ ì±Œë¦°ì§€ ë°ì´í„° ì„¸íŠ¸ ì œê³µë©ë‹ˆë‹¤
- ì±Œë¦°ì§€ ê¸°ê°„ì´ ëë‚˜ë©´ ì •í™•ë„ì™€ ì‹œê°„ ì¸¡ë©´ì—ì„œ ê· ì£¼ ë‹¹ 500 ê°œì˜ ê²Œë†ˆ(ì´ 2000ê°œì˜ ê²Œë†ˆ)ì´ í¬í•¨ëœ hold-out ë°ì´í„° ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì†”ë£¨ì…˜ì„ í‰ê°€í•˜ê²Œ ë©ë‹ˆë‹¤
- **ì•”í˜¸í™”ëœ ê²Œë†ˆ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¥˜ ì‘ì—…ì„ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰ í•  ìˆ˜ ìˆëŠ” ë¶„ë¥˜ê¸°ë¥¼ ê°œë°œí•˜ëŠ”ê²ƒì´ ì´ë²ˆ ì±Œë¦°ì§€ì˜ ê³¼ì œë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**
- ì €í¬íŒ€ì€ ë°ì´í„°ë¥¼ ì•”í˜¸í™”í•˜ê¸° ì „ì— preprocessor.cpp ì½”ë“œë¥¼ í†µí•´ ì „ì²˜ë¦¬ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤
- ë˜í•œ ì•„ë˜ì˜ ì¡°ê±´ì´ ì¶©ì¡±ë˜ëŠ” í•œ ì•”í˜¸í™” ë˜ê¸° ì „ì— ì…ë ¥ ê²Œë†ˆì˜ ì „ì²˜ë¦¬ì— ëŒ€í•œ ì œì•½ì´ ì—†ê¸° ë•Œë¬¸ì— ì¡°ê±´ì„ ë§ì¶”ê¸° ìœ„í•´ ë„ì»¤ë¥¼ í†µí•´ í™˜ê²½ì„ ì œì•½ í•˜ì˜€ìŠµë‹ˆë‹¤
    1. ì „ì²˜ë¦¬ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ evaluation ë°ì´í„° ì…‹ì˜ ê²Œë†ˆ 2000ê°œì— ëŒ€í•´ 1GBë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šì•„ì•¼í•¨
    2. ì „ì²˜ë¦¬ ì‹œê°„ ì‚¬ìš©ëŸ‰ì€ 1CPU ì½”ì–´ì—ì„œ 2000ê°œì˜ ê²Œë†ˆì— ëŒ€í•´ 1ë¶„ì„ ì´ˆê³¼í•˜ì§€ ì•ŠìŒ - Intel Xeon CPUì—ì„œ OSì— ëŒ€í•œ ê°„ì„­ì—†ì´ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰. ì¦‰, ë°©ë²•ì„ í‰ê°€í•˜ëŠ” ë™ì•ˆ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

### Challenge Dataset and Files

- ì±Œë¦°ì§€ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ê²Œë†ˆì´ í¬í•¨ë˜ì–´ìˆìŠµë‹ˆë‹¤
- 4ê°œì˜ ë‹¤ë¥¸ ë°”ì´ëŸ¬ìŠ¤ ê· ì£¼ì— ëŒ€í•œ ê²Œë†ˆ ì„œì—´ì„ í¬í•¨í•˜ëŠ” í•˜ë‚˜ì˜ FASTA íŒŒì¼ì´ ì œê³µë©ë‹ˆë‹¤
    - FASTA í¬ë§·ì€ ì—¼ê¸°ì„œì—´ í˜¹ì€ ë‹¨ë°±ì§ˆì„œì—´ì„ ë¬¸ìì—´ë¡œ í‘œí˜„í•œ ê²ƒ
- ê° ë°”ì´ëŸ¬ìŠ¤ ê²Œë†ˆì˜ í˜•íƒœëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤

![image](https://user-images.githubusercontent.com/29485153/143258322-c1a0bb1f-a495-4919-842d-3c72a51edaf7.png)

### Encryption Requirement

- HE ë°©ì‹ì˜ ë³´ì•ˆ ìˆ˜ì¤€ì€ 128ë¹„íŠ¸ ì´ìƒìœ¼ë¡œ ì„¤ì • ë˜ì–´ì•¼í•©ë‹ˆë‹¤

### Evaluation Environment

- ëª¨ë“  ì œì¶œì€ Docker ì»¨í…Œì´ë„ˆë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë¨. ì»¨í…Œì´ë„ˆëŠ” í‰ê°€ì—ì„œ CPU ì½”ì–´ 4ê°œ (Intel Xeon Platinum 8180 CPU @ 2.50GHz), 32GB ë©”ëª¨ë¦¬ ë° 500GB ìŠ¤í† ë¦¬ì§€ë¡œ ì œí•œë©ë‹ˆë‹¤
- ì œì¶œ íŒŒì¼ì„ ë„ì»¤ í˜•íƒœë¡œ ì œì¶œí•´ì„œ ì œí•œ ì‚¬í•­ì„ ë§ì¶°ì£¼ì—ˆìŠµë‹ˆë‹¤

### D**ocker hub link**

ì•„ë˜ì˜ ë„ì»¤ í—ˆë¸Œ ë‚´ì— README íŒŒì¼ì„ í†µí•´ ë„ì»¤ ì‹¤í–‰ ë°©ë²•ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ëª¨ë¸ì— ëŒ€í•œ ë„ì»¤ : [https://hub.docker.com/repository/docker/gnadi4/model](https://hub.docker.com/repository/docker/gnadi4/model)Â 

ì „ì²˜ë¦¬ê¸°ì— ëŒ€í•œ ë„ì»¤ : [https://hub.docker.com/repository/docker/gnadi4/preprocessing](https://hub.docker.com/repository/docker/gnadi4/preprocessing)

---

---

### ***ì œì¶œ ì½”ë“œì˜ êµ¬í˜„ë°©ì‹ ë° ë™ì‘ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤(ì˜ë¬¸)

# Homomorphic Encryption-based Secure Viral Strain Classification

### Team name : Data Science Dept. in SeoulTech

## Method

  We used the open version of the HEaaN library$^{1}$ the implementation of CKKS$^{2}$ scheme, and logistic regression for multi-label classification in this task. Our solution consists of three steps: preprocessing, training, and inference

![image](https://user-images.githubusercontent.com/29485153/143258374-a724e7bd-1153-4ab0-86e0-0942cb3f9ebc.png)

## **2.1 Notation**

![image](https://user-images.githubusercontent.com/29485153/143258396-b0bd03a1-20a5-4bbf-b3ef-3602951db46a.png)

## 2.2 Preprocessing

  To vectorize a viral genome, we firstly extract all subsequences of a predefined length from full genomes given for training. We consider the subsequences of length 4 to manage the number of features based on the empirical observation: the shorter the subsequence length, the less the number of features. For example, when the subsequence length is set to 1,000, 24,556 unique subsequences are obtained, while if it is set to 4, the number of generated unique subsequences is 980. These 980 subsequences are denoted as the candidate features.

  Given the generated candidate features, a genome sequence can be transformed into a vector with several strategies such as binary encoding, counting, etc. In this work, we employ a counting approach to vectorize given genomes. Specifically, the entire genome sequence is represented as a vector of integers where each element denotes the number of subsequences matched with the corresponding feature.

  With the candidate subsequences, we conduct a feature selection process to further reduce the number of features to be used for building a classification model. For the feature selection, we use the RFE method implemented in the sklearn library. RFE (abbreviation of "Recursive Feature Elimination") is a process of selecting the predefined number of optimal features by removing the most unnecessary features one by one based on the importance of each feature with a linear model (i.e., logistic regression explained in the subsequent section). In this feature selection process, we consider 40, 60, and 120 as the number of features to be selected.

  To find the optimal number of features in terms of predictive performance, randomly selected 70% genomes were used for training, and the remaining 30% genomes were used to measure the performance. We repeated this procedure 100 times for a rigorous comparison. From the experiments, we observed that the selected 120 features through RFE show slightly the better performance.

![image](https://user-images.githubusercontent.com/29485153/143258424-668bc3eb-f0ed-4997-b7ac-0c50b58e774f.png)
The final selected 120 features are listed as follows:

![image](https://user-images.githubusercontent.com/29485153/143258442-9f12dc24-ee81-41bc-99e9-41c8768b8a84.png)

## **2.3 Training Logistic-regression Model**

  As a classification model, we employ logistic regression. It outputs class probabilities given a vectorized input (i.e., count vector). Given an input x, a logistic regression model produces the logit values with k weight vectors associated with each class as:
  
![image](https://user-images.githubusercontent.com/29485153/143258463-5b98a421-aec8-49c7-a4a8-5bd3750cfa6e.png)
  
  These continuous logit values are transformed into class probabilities with a softmax function as follows:

![image](https://user-images.githubusercontent.com/29485153/143258504-ffe1611c-9bf4-4fc7-a1c3-d1c5d7971133.png)

  The softmax transformation requires a lot of computational costs with encrypted data. To avoid this issue, we assume that the logit value represents class preference since each logit value corresponds to the distance between a decision hyperplane and a data point. We empirically confirmed that using the logit values does not degrade predictive performance.

  We evaluated our selected 120 features and a logistic regression model again with different random seeds. In this experiment, the given data were divided into 70% for training and 30% for validation. The results are summarized as Table 2:

![image](https://user-images.githubusercontent.com/29485153/143258529-b9d02442-3cb9-4bbb-9e80-1eb9a3921436.png)

From these results, we confirmed again that our approach consistently shows almost perfect classification performance over different data splits.

## **2.4 Inference Phase**

This part describes how to obtain the classification result through preprocessing of input test data, encryption of the preprocessed data, logit calculation with the encrypted input and the trained model, and decryption of the result.

### **2.4.1 Preprocessing**

Input data is represented as [Figure 3] and the detailed preprocessing steps areÂ  provided hereafter. The input fasta file contains 2000 genome samples to be classified. We suppose Xi, is the i-th sample as in the right side of [Figure 3]. Each Xi consists of ğ‘™ letters out of A, C, G, T, N, etc, where ğ‘™ is around 28000. To build up the input vector, we count the number of occurrences of each element in Sseq from Xi , where Sseqcontains all possible 4 letter combinations of A, C, G, and T. The result of counting is placed in a vector of 256 elements, where each position is mapped to a specific combination of the letters. For simplicity, we assign the position of each combination based on its lexicographic order in Sseq. We suppose the resultant vector made with Xi as mi. This process is depicted in [Figure 4]. In the figure, countXi(seq) for any seq âˆˆSseq refers to the counting result for seq in Xi. Please be aware that the overlapped sequence is ignored in counting. E. g.,Â  as in [Figure 5-â“], â€œACGT â€ appears 2 times inXi without overlap, resulted in countXi(ACGT)=2, whereas â€œGTAGâ€ appears 2 times but â€˜Gâ€™ is overlapped, thus countXi(GTAG)=1 as shown in [Figure 5-â“‘]. Also, we do not count any sequence that contains â€˜Nâ€™ and the other letters than â€˜Aâ€™, â€˜Câ€™, â€˜Gâ€™, and ,â€™Tâ€™, such as â€˜TAGNâ€™.

The process of obtaining mi from Xi is the main routine for pre-processing. We denote it as fcount(Xi). The detailed process of fcount(Xi) is given in [Figure 7]. The subroutines used in fcount(Xi) are introducedÂ  in [Figure 6].

![image](https://user-images.githubusercontent.com/29485153/143258552-ae8e7c1f-dfe8-4a32-8734-dbe4b9da70d0.png)

![image](https://user-images.githubusercontent.com/29485153/143258574-e2c3f015-3ba7-4f9e-9838-265ee6e2800f.png)

For the efficiency of preprocessing, fcount(Xi) scans Xi only once. For each seq cut from the beginning, the lexicographical order of seq is obtained through fidx(seq). To avoid counting overlapping seq, it stores the three previous seq in p0,p1, and p2, respectively. If seq is an element of Sseq and not overlapped, store it in the array as an index obtained through fidx(seq).

![image](https://user-images.githubusercontent.com/29485153/143258597-c3570d6c-e20c-44cd-8e8c-4381f358ab16.png)

The entire process of preprocessing is to perform fcount(Xii) for all Xis . [Figure 8] represents the results of the entire preprocessing process when a part of the sample data is provided as an input. This matrix is exported in .csv file format without header. [Figure 9] is an example of result csv.sudo

![image](https://user-images.githubusercontent.com/29485153/143258619-8321023d-b786-4c76-9647-953b74bef7d0.png)

![image](https://user-images.githubusercontent.com/29485153/143258653-90efd99d-e390-4f07-a373-e5bda91ecab2.png)

### **2.4.2 Encryption of pre-processed data**

We use the open version of the HEaaN library [1], the implementation of CKKS[2] scheme where parameters are N=212, log q= 101, log p =45 with (uniformly random) signed binary secret. We use this parameterÂ  to meet both security and time constraints of the competition calculation. We use 2048 slots. As a result of the training step in Section 2.3, we have 4 model vectors of w1, w2, w3, and w4. By performing the inner product operation with the input vector respectively, we can obtain the logit value for each class.

The result of section 2.4.1 is mi whose length is 256. Because mi needs to be multiplied by w1~ , respectively, we make a new vector that contains mi by 4 times in a duplicated manner. Then, it is encrypted. Since the size of the new vector is 1024, we can put two mis in a ciphertext. This process is depicted in [Figure 10].Â  One additional operation conducted before encryption is that every element in the new vector is multiplied by 0.001 to make the range of its value in [0, 1].

Finally, these 1000 ciphertexts will be delivered to the next step to compute the logit value.

![image](https://user-images.githubusercontent.com/29485153/143258688-0e99c585-8e6c-4b2e-a442-e2af4fc9b54b.png)

### **2.4.3 Computing logit value with encrypted inputs**

The 1000 ciphertexts generated in section 2.4.2 are processed in this part. Please remember that there are 120 features that were selected for training. The main task in this step is how the values of the selected features are extracted from each input ciphertext to compute the logit value for each inputmi. Because we know which slotâ€™s value in a ciphertext needs to be multiplied with which coefficient value in the model, we can construct a plaintext polynomial that has the model coefficient values in the right positions.

[Figure 11] shows an example of model vectors w1~ w4. The â€˜Featureâ€™ row shows what featuresâ€™ count values should be multiplied to the model coefficients. Since mi is a vector of 256, we put the coefficient values for each model in the certain positionsÂ  of a vector of 256 elements either so that they can be multiplied with the correct count values in mi. This process is shown in [Figure 12].

![image](https://user-images.githubusercontent.com/29485153/143258716-5eca2957-9044-4729-a22b-af41f8803b2f.png)

Because each ciphertext contains 2 mis, we make a model coefficient vector using the model vectors as in [Figure 13]. After converting the coefficient vector into a plaintext polynomial, it is multiplied with each input ciphertext.

![image](https://user-images.githubusercontent.com/29485153/143258748-203431cc-cbc7-4dea-8b07-15b28da5bb9b.png)

![image](https://user-images.githubusercontent.com/29485153/143258769-e10a5602-a989-4b2b-bb96-52f07f75577a.png)

In addition, the number of intercept in the trained logistic regression model is 4, one for each class. If the intercepts for each class are assumed to be {intercept1, intercept2,intercept3, intercept4}, as shown in [Figure 14], since there are two mis in a ciphertext, o we have to make a polynomial including a pair of all intercepts in the following order: {intercept1, intercept2, intercept3, intercept4, intercept1, intercept2, intercept3, intercept4} to process them at once . Also, zero values are put in the slots where the values of the intercepts are not stored. It is added to the result in the next step after left rotations are completed.

![image](https://user-images.githubusercontent.com/29485153/143258807-4e983652-ff9e-4171-942b-4963674c4666.png)

In order to store the logit values in the slot positions of multiples from 0 to 256th index, after multiplication, a set of left rotate operations are performed as in [Figure 15].

![image](https://user-images.githubusercontent.com/29485153/143258834-0c6e8be8-6300-4961-9464-d4eb63e4166f.png)

### **2.4.4 Obtaining the classification result**

Finally, as shown in [Figure 16], the result of logistic regression is obtained as a result of the previous step. The logit value for class i is in the 256*(i-1)th slot or 1024+256*(i-1)th slot in the ciphertext. Thisprocess is repeated for all every ciphertext from the step in Section 2.4.3.

![image](https://user-images.githubusercontent.com/29485153/143258848-f8158478-f455-498b-b645-255c76554549.png)

As described in the Section 2.3, the values can be transformed into class probabilities with a softmax function. However, the softmax transformation requires a lot of computational costs with encrypted data. Moreover, from the result in an unencrypted state, we checked that the values without executing softmax consistently give almost perfect classification performance over different data splits. So without running the softmax function, the ciphertext is delivered to be decrypted, and after decryption, it is stored in the form of .csv as shown in [Figure 17] for convenience in checking the result. The first line of the output csv file is the name of each class( [B.1.427] [B.1.1.7] [P.1] [B.1.526] ) and after line, the order of result.csv file follows the order of the Xi.

![image](https://user-images.githubusercontent.com/29485153/143258861-cdfc33f5-c165-4bae-9dcf-0a2abc5c9b7f.png)

## **3 Experimental Result**

We use the open version of the HEaaN library [1], the implementation of CKKS[2] scheme, and the parameters are N=212, log q= 101, log p =45 with (uniformly random) signed binary secret. log qis smaller than 103 which satisfies more than 128-bit security according to [Table 3].

![image](https://user-images.githubusercontent.com/29485153/143258880-3318565c-7d74-4076-ba57-cfa925d5e45b.png)

Our implementation is executed on Ubuntu 20.04 , Intel Xeon Platinum 8259CL CPU @ 2.50GHz processor with 8 threads. In addition, out of 7461 samples from which 539 duplicates were removed from a given Challenge.fa file,70% of them were used for the training data set(70%), and the other 30% were used for the test data set . The detailed results are as follows.

### Result

The preprocess stage measurement was carried out using only 1 CPU core. The result is shown in [Figure 18].

![image](https://user-images.githubusercontent.com/29485153/143258902-5871c669-db86-4292-bb62-70717ed5ac6b.png)

To evaluate our model, we used 10-fold cross validation in an encrypted state. The result is shown in [Figure 19].

![image](https://user-images.githubusercontent.com/29485153/143258923-e4a2f16c-3546-4d61-95c9-c35a1f4be93f.png)

The round trip of the encryption process is shown in [Figure 20]. The total memory usage is 1261MB, of which 1072MB is used to keep the ciphertext. The total storage usage is the size of docker.

![image](https://user-images.githubusercontent.com/29485153/143258934-2727a931-e54d-424f-ab07-7eef7ac74db0.png)

As mentioned earlier, our experimental environments Intel Xeon Platinum 8259CL CPU @ 2.50GHz processor with 8 threads. However, the actual testing environment is Intel Xeon Platinum 8180 CPU @ 2.50GHz with 4 cpu cores. As shown in Figure 20, In single core, the Geekbench 4 core score difference between both environments is 9%. In multi-core, the performance gap is increased to 45%.

![image](https://user-images.githubusercontent.com/29485153/143258959-474a1f19-7dda-48fd-b9cd-d62f809b9df5.png)

Considering the difference of the execution environment and the number of samples classified, we expect the running time to classify 2000 samples as shown in [Table 4].

![image](https://user-images.githubusercontent.com/29485153/143258970-abc47943-281e-4250-a4ef-d8d9f88b29bd.png)

**References**

[1] suncrypto/HEAAN git hub page [https://github.com/snucrypto/HEAAN](https://github.com/snucrypto/HEAAN)

[2] [CKKS17] Cheon, J. H., Kim, A., Kim, M., & Song, Y. (2017, December). Homomorphic encryption for arithmetic of approximate numbers. In International Conference on the Theory and Application of Cryptology and Information Security (pp. 409-437). Springer, Cham.

[3] SECURITY OF HOMOMORPHIC ENCRYPTION July 13-14, 2017, hosted at Microsoft Research in Redmond. http://homomorphicencryption.org/white_papers/security_homomorphic_encryption_white_paper.pdf

[4] GadgetVersus site, compare Intel Xeon Platinum 8259CL vs Intel Xeon Platinum 8180.

https://gadgetversus.com/processor/intel-xeon-platinum-8259cl-vs-intel-xeon-platinum-8180/
